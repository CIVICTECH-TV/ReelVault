use std::collections::HashMap;
use std::path::Path;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use serde::{Deserialize, Serialize};
use tauri::{command, State, AppHandle, Emitter};
use tokio::sync::mpsc;
use tokio::time::sleep;
use uuid::Uuid;


use crate::commands::aws_auth::AwsCredentials;
use crate::commands::metadata::create_file_metadata;

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¢ã‚¤ãƒ†ãƒ ã®çŠ¶æ…‹
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum UploadStatus {
    Pending,
    InProgress,
    Completed,
    Failed,
    Paused,
    Cancelled,
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¢ã‚¤ãƒ†ãƒ 
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UploadItem {
    pub id: String,
    pub file_path: String,
    pub file_name: String,
    pub file_size: u64,
    pub s3_key: String,
    pub status: UploadStatus,
    pub progress: f64,
    pub uploaded_bytes: u64,
    pub speed_mbps: f64,
    pub eta_seconds: Option<u64>,
    pub created_at: String,
    pub started_at: Option<String>,
    pub completed_at: Option<String>,
    pub error_message: Option<String>,
    pub retry_count: u32,
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é€²æ—æƒ…å ±
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UploadProgress {
    pub item_id: String,
    pub uploaded_bytes: u64,
    pub total_bytes: u64,
    pub percentage: f64,
    pub speed_mbps: f64,
    pub eta_seconds: Option<u64>,
    pub status: UploadStatus,
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨­å®š
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UploadConfig {
    pub aws_credentials: AwsCredentials,
    pub bucket_name: String,
    pub max_concurrent_uploads: usize,
    pub chunk_size_mb: u64,
    pub retry_attempts: u32,
    pub timeout_seconds: u64,
    pub auto_create_metadata: bool,
    pub s3_key_prefix: Option<String>,
    
    // ğŸ¯ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ç”¨ã®åˆ¶é™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    pub max_concurrent_parts: usize,        // ãƒãƒ£ãƒ³ã‚¯ãƒ¬ãƒ™ãƒ«ä¸¦åˆ—åº¦ï¼ˆç„¡æ–™ç‰ˆ: 1, ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ç‰ˆ: 4-8ï¼‰
    pub adaptive_chunk_size: bool,          // å‹•çš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆç„¡æ–™ç‰ˆ: false, ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ç‰ˆ: trueï¼‰
    pub min_chunk_size_mb: u64,            // æœ€å°ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆç„¡æ–™ç‰ˆ: 5MBå›ºå®šï¼‰
    pub max_chunk_size_mb: u64,            // æœ€å¤§ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆç„¡æ–™ç‰ˆ: 5MBå›ºå®šï¼‰
    pub bandwidth_limit_mbps: Option<f64>,  // å¸¯åŸŸåˆ¶é™ï¼ˆç„¡æ–™ç‰ˆ: ãªã—, ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ç‰ˆ: è¨­å®šå¯èƒ½ï¼‰
    pub enable_resume: bool,                // ä¸­æ–­ãƒ»å†é–‹æ©Ÿèƒ½ï¼ˆç„¡æ–™ç‰ˆ: false, ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ç‰ˆ: trueï¼‰
    pub tier: UploadTier,                   // æ©Ÿèƒ½ãƒ†ã‚£ã‚¢
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ãƒ†ã‚£ã‚¢
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq)]
pub enum UploadTier {
    Free,     // ç„¡æ–™ç‰ˆ
    Premium,  // ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ç‰ˆ
}

impl UploadConfig {
    /// çµ±ä¸€ã•ã‚ŒãŸã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨­å®šã‚’ç”Ÿæˆ
    /// @param aws_credentials AWSèªè¨¼æƒ…å ±
    /// @param bucket_name S3ãƒã‚±ãƒƒãƒˆå
    /// @param tier ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ãƒ†ã‚£ã‚¢ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Premiumï¼‰
    pub fn new(aws_credentials: AwsCredentials, bucket_name: String, tier: UploadTier) -> Self {
        // åŸºæœ¬è¨­å®šï¼ˆå…±é€šï¼‰
        let base_config = Self {
            aws_credentials,
            bucket_name,
            auto_create_metadata: true,
            s3_key_prefix: Some("uploads".to_string()),
            tier,
            // ä»¥ä¸‹ã¯ãƒ†ã‚£ã‚¢åˆ¥ã§ä¸Šæ›¸ã
            max_concurrent_uploads: 1,
            chunk_size_mb: 5,
            retry_attempts: 3,
            timeout_seconds: 600,
            max_concurrent_parts: 1,
            adaptive_chunk_size: false,
            min_chunk_size_mb: 5,
            max_chunk_size_mb: 5,
            bandwidth_limit_mbps: None,
            enable_resume: false,
        };

        match tier {
            UploadTier::Free => Self {
                // ç„¡æ–™ç‰ˆåˆ¶é™
                max_concurrent_uploads: 1,      // 1ãƒ•ã‚¡ã‚¤ãƒ«ãšã¤
                chunk_size_mb: 5,               // 5MBå›ºå®š
                retry_attempts: 3,              // 3å›ã¾ã§
                timeout_seconds: 600,           // 10åˆ†
                max_concurrent_parts: 1,        // ãƒãƒ£ãƒ³ã‚¯ã‚‚1ã¤ãšã¤ï¼ˆé †æ¬¡å‡¦ç†ï¼‰
                adaptive_chunk_size: false,     // å›ºå®šã‚µã‚¤ã‚º
                min_chunk_size_mb: 5,          // 5MBå›ºå®š
                max_chunk_size_mb: 5,          // 5MBå›ºå®š
                bandwidth_limit_mbps: None,     // åˆ¶é™ãªã—
                enable_resume: false,           // å†é–‹æ©Ÿèƒ½ãªã—
                ..base_config
            },
            UploadTier::Premium => Self {
                // ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ç‰ˆæ©Ÿèƒ½
                max_concurrent_uploads: 8,      // 8ãƒ•ã‚¡ã‚¤ãƒ«åŒæ™‚
                chunk_size_mb: 10,              // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10MB
                retry_attempts: 10,             // 10å›ã¾ã§
                timeout_seconds: 1800,          // 30åˆ†
                max_concurrent_parts: 8,        // 8ãƒãƒ£ãƒ³ã‚¯ä¸¦åˆ—
                adaptive_chunk_size: true,      // å‹•çš„æœ€é©åŒ–
                min_chunk_size_mb: 5,          // æœ€å°5MBï¼ˆS3åˆ¶é™æº–æ‹ ï¼‰
                max_chunk_size_mb: 100,        // æœ€å¤§100MB
                bandwidth_limit_mbps: None,     // åˆ¶é™ãªã—ï¼ˆè¨­å®šå¯èƒ½ï¼‰
                enable_resume: true,            // å†é–‹æ©Ÿèƒ½ã‚ã‚Š
                ..base_config
            }
        }
    }
    
    /// ç¾åœ¨ã®è¨­å®šãŒç„¡æ–™ç‰ˆåˆ¶é™å†…ã‹ãƒã‚§ãƒƒã‚¯
    pub fn validate_free_tier_limits(&self) -> Result<(), String> {
        if self.tier == UploadTier::Free {
            if self.max_concurrent_uploads > 1 {
                return Err("ç„¡æ–™ç‰ˆã§ã¯åŒæ™‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯1ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§ã§ã™".to_string());
            }
            if self.max_concurrent_parts > 1 {
                return Err("ç„¡æ–™ç‰ˆã§ã¯ãƒãƒ£ãƒ³ã‚¯ä¸¦åˆ—å‡¦ç†ã¯ã§ãã¾ã›ã‚“".to_string());
            }
            if self.adaptive_chunk_size {
                return Err("ç„¡æ–™ç‰ˆã§ã¯å‹•çš„ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¯åˆ©ç”¨ã§ãã¾ã›ã‚“".to_string());
            }
            if self.chunk_size_mb != 5 || self.min_chunk_size_mb != 5 || self.max_chunk_size_mb != 5 {
                return Err("ç„¡æ–™ç‰ˆã§ã¯ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¯5MBå›ºå®šã§ã™".to_string());
            }
        }
        Ok(())
    }
}



/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã®ç®¡ç†
#[derive(Debug)]
pub struct UploadQueue {
    pub items: Vec<UploadItem>,
    pub active_uploads: HashMap<String, UploadProgress>,
    pub config: Option<UploadConfig>,
    pub is_processing: bool,
    pub total_uploaded_bytes: u64,
    pub total_files_uploaded: u64,
    /// å³æ ¼ãªåŒæ™‚å®Ÿè¡Œåˆ¶å¾¡ã®ãŸã‚ã®å°‚ç”¨ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
    pub active_upload_count: usize,
}

impl UploadQueue {
    pub fn new() -> Self {
        Self {
            items: Vec::new(),
            active_uploads: HashMap::new(),
            config: None,
            is_processing: false,
            total_uploaded_bytes: 0,
            total_files_uploaded: 0,
            active_upload_count: 0,
        }
    }
    
    /// å®‰å…¨ãªåŒæ™‚å®Ÿè¡Œæ•°å–å¾—
    pub fn get_active_upload_count(&self) -> usize {
        // è¤‡æ•°ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¦æœ€ã‚‚æ­£ç¢ºãªå€¤ã‚’è¿”ã™
        let in_progress_count = self.items.iter()
            .filter(|item| item.status == UploadStatus::InProgress)
            .count();
        let active_uploads_count = self.active_uploads.len();
        
        // æœ€å¤§å€¤ã‚’ä½¿ç”¨ï¼ˆã‚ˆã‚Šä¿å®ˆçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        std::cmp::max(
            std::cmp::max(in_progress_count, active_uploads_count),
            self.active_upload_count
        )
    }
    
    /// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹æ™‚ã®çŠ¶æ…‹æ›´æ–°
    pub fn start_upload(&mut self, item_id: &str) -> Result<(), String> {
        if let Some(config) = &self.config {
            let current_active = self.get_active_upload_count();
            
            // ç„¡æ–™ç‰ˆã®å³æ ¼ãªåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if config.max_concurrent_uploads == 1 && current_active > 0 {
                return Err(format!("ç„¡æ–™ç‰ˆã§ã¯åŒæ™‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯1ã¤ã¾ã§ã§ã™ã€‚ç¾åœ¨ã‚¢ã‚¯ãƒ†ã‚£ãƒ–: {}", current_active));
            }
            
            if current_active >= config.max_concurrent_uploads {
                return Err(format!("åŒæ™‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ•°ã®ä¸Šé™ã«é”ã—ã¦ã„ã¾ã™: {}/{}", 
                                 current_active, config.max_concurrent_uploads));
            }
        }
        
        // çŠ¶æ…‹ã‚’æ›´æ–°
        if let Some(item) = self.items.iter_mut().find(|i| i.id == item_id) {
            item.status = UploadStatus::InProgress;
            item.started_at = Some(chrono::Utc::now().to_rfc3339());
            self.active_upload_count += 1;
            
            log::info!("Upload started: {} (active count: {})", item_id, self.active_upload_count);
            Ok(())
        } else {
            Err(format!("Upload item not found: {}", item_id))
        }
    }
    
    /// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†æ™‚ã®çŠ¶æ…‹æ›´æ–°
    pub fn complete_upload(&mut self, item_id: &str, success: bool, error_msg: Option<String>) {
        log::info!("ğŸ”§ complete_upload called: {} (success: {})", item_id, success);
        
        // ã‚¢ã‚¤ãƒ†ãƒ ã®ç¾åœ¨ã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
        let current_status = self.items.iter()
            .find(|i| i.id == item_id)
            .map(|i| i.status.clone());
        
        if let Some(status) = &current_status {
            if *status == UploadStatus::Completed {
                log::info!("âš ï¸  Upload already completed, skipping duplicate cleanup: {}", item_id);
                return;
            }
        }
        
        // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¸›ã‚‰ã™ï¼ˆé‡è¤‡æ¸›ç®—ã‚’é˜²ãï¼‰
        let was_active = self.active_uploads.contains_key(item_id) || 
                        current_status == Some(UploadStatus::InProgress);
        
        log::info!("ğŸ” Cleanup state check - was_active: {}, active_uploads contains: {}, current_status: {:?}", 
                   was_active, self.active_uploads.contains_key(item_id), current_status);
        
        if was_active && self.active_upload_count > 0 {
            self.active_upload_count -= 1;
            log::info!("ğŸ”½ Active upload count decreased: {} -> {}", 
                       self.active_upload_count + 1, self.active_upload_count);
        }
        
        // active_uploadsã‹ã‚‰å‰Šé™¤ï¼ˆæˆåŠŸãƒ»å¤±æ•—ã«é–¢ã‚ã‚‰ãšå¿…ãšå‰Šé™¤ï¼‰
        let removed = self.active_uploads.remove(item_id);
        log::info!("ğŸ—‘ï¸  Removing from active_uploads: {} (was present: {})", item_id, removed.is_some());
        
        // ã‚¢ã‚¤ãƒ†ãƒ ã®çŠ¶æ…‹ã‚’æ›´æ–°
        if let Some(item) = self.items.iter_mut().find(|i| i.id == item_id) {
            if success {
                item.status = UploadStatus::Completed;
                item.completed_at = Some(chrono::Utc::now().to_rfc3339());
                item.progress = 100.0;
                self.total_files_uploaded += 1;
                self.total_uploaded_bytes += item.file_size;
                log::info!("âœ… Upload marked as completed: {} ({})", item.file_name, item_id);
            } else {
                item.status = UploadStatus::Failed;
                item.error_message = error_msg;
                log::error!("âŒ Upload marked as failed: {} ({})", item.file_name, item_id);
            }
        }
        
        log::info!("ğŸ“Š Upload completion summary - Active count: {}, Active uploads: {}, Items in progress: {}", 
                   self.active_upload_count, 
                   self.active_uploads.len(),
                   self.items.iter().filter(|i| i.status == UploadStatus::InProgress).count());
    }
    
    /// ç„¡æ–™ç‰ˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
    pub fn check_free_tier_limits(&self, new_files_count: usize) -> Result<(), String> {
        if let Some(config) = &self.config {
            if config.max_concurrent_uploads == 1 {
                // ç„¡æ–™ç‰ˆã®å ´åˆ
                let current_active = self.get_active_upload_count();
                let pending_count = self.items.iter()
                    .filter(|item| item.status == UploadStatus::Pending)
                    .count();
                
                if current_active > 0 && new_files_count > 0 {
                    return Err(format!(
                        "ç„¡æ–™ç‰ˆã§ã¯åŒæ™‚å‡¦ç†ã«åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚ç¾åœ¨å‡¦ç†ä¸­: {}ãƒ•ã‚¡ã‚¤ãƒ«ã€‚å®Œäº†å¾Œã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚",
                        current_active
                    ));
                }
                
                if pending_count > 0 && new_files_count > 0 {
                    return Err(format!(
                        "ç„¡æ–™ç‰ˆã§ã¯å¾…æ©Ÿä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã€æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã§ãã¾ã›ã‚“ã€‚å¾…æ©Ÿä¸­: {}ãƒ•ã‚¡ã‚¤ãƒ«",
                        pending_count
                    ));
                }
            }
        }
        Ok(())
    }
}

pub type UploadQueueState = Arc<Mutex<UploadQueue>>;

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çµ±è¨ˆ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UploadStatistics {
    pub total_files: u64,
    pub completed_files: u64,
    pub failed_files: u64,
    pub pending_files: u64,
    pub in_progress_files: u64,
    pub total_bytes: u64,
    pub uploaded_bytes: u64,
    pub average_speed_mbps: f64,
    pub estimated_time_remaining: Option<u64>,
}

/// ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®çµæœ
#[derive(Debug, Serialize, Deserialize)]
pub struct FileSelection {
    pub selected_files: Vec<String>,
    pub total_size: u64,
    pub file_count: u32,
}

/// S3ã‚­ãƒ¼ç”Ÿæˆè¨­å®š
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct S3KeyConfig {
    pub prefix: Option<String>,
    pub use_date_folder: bool,
    pub preserve_directory_structure: bool,
    pub custom_naming_pattern: Option<String>,
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã‚’åˆæœŸåŒ–
#[command]
pub async fn initialize_upload_queue(
    config: UploadConfig,
    queue_state: State<'_, UploadQueueState>,
) -> Result<String, String> {
    // ğŸ” å—ä¿¡ã—ãŸè¨­å®šã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    log::info!("ğŸ”§ å—ä¿¡ã—ãŸè¨­å®š: tier={:?}, chunk_size_mb={}, max_concurrent_uploads={}, max_concurrent_parts={}, adaptive_chunk_size={}, min_chunk_size_mb={}, max_chunk_size_mb={}", 
               config.tier, config.chunk_size_mb, config.max_concurrent_uploads, config.max_concurrent_parts, 
               config.adaptive_chunk_size, config.min_chunk_size_mb, config.max_chunk_size_mb);
    
    let mut queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    queue.config = Some(config);
    queue.items.clear();
    queue.active_uploads.clear();
    queue.is_processing = false;
    
    log::info!("Upload queue initialized successfully");
    Ok("Upload queue initialized".to_string())
}

/// ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹ã
#[command]
pub async fn open_file_dialog(
    app_handle: AppHandle,
    multiple: bool,
    _file_types: Option<Vec<String>>,
) -> Result<FileSelection, String> {
    use tauri_plugin_dialog::DialogExt;
    use std::sync::mpsc;
    
    log::info!("Opening file dialog, multiple: {}", multiple);
    
    let (tx, rx) = mpsc::channel();
    
    if multiple {
        // è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        app_handle.dialog().file().pick_files(move |file_paths| {
            let _ = tx.send(file_paths);
        });
    } else {
        // å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        app_handle.dialog().file().pick_file(move |file_path| {
            let paths = file_path.map(|p| vec![p]);
            let _ = tx.send(paths);
        });
    }
    
    // çµæœã‚’å¾…æ©Ÿ
    let selected_paths = match rx.recv() {
        Ok(Some(paths)) => paths,
        Ok(None) => {
            log::info!("No files selected");
            return Ok(FileSelection {
                file_count: 0,
                total_size: 0,
                selected_files: vec![],
            });
        }
        Err(e) => {
            return Err(format!("Failed to receive file dialog result: {}", e));
        }
    };
    
    let mut selected_files = Vec::new();
    let mut total_size = 0u64;
    
    for path in selected_paths {
        let path_str = path.to_string();
        selected_files.push(path_str.clone());
        
        if let Ok(metadata) = std::fs::metadata(&path_str) {
            total_size += metadata.len();
        }
    }
    
    log::info!("File dialog result: {} files selected, total size: {} bytes", 
               selected_files.len(), total_size);
    
    Ok(FileSelection {
        file_count: selected_files.len() as u32,
        total_size,
        selected_files,
    })
}

/// ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
#[command]
pub async fn add_files_to_upload_queue(
    file_paths: Vec<String>,
    s3_key_config: S3KeyConfig,
    queue_state: State<'_, UploadQueueState>,
) -> Result<String, String> {
    let mut queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    // ç„¡æ–™ç‰ˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
    queue.check_free_tier_limits(file_paths.len())?;
    
    let mut added_files = Vec::new();
    
    for file_path in file_paths {
        let path = Path::new(&file_path);
        if !path.exists() {
            log::warn!("File does not exist, skipping: {}", file_path);
            continue;
        }
        
        let metadata = std::fs::metadata(&path)
            .map_err(|e| format!("Failed to get file metadata for {}: {}", file_path, e))?;
        
        let file_name = path.file_name()
            .and_then(|s| s.to_str())
            .unwrap_or("Unknown")
            .to_string();
        
        let s3_key = generate_s3_key(&file_path, &s3_key_config)?;
        
        let item = UploadItem {
            id: Uuid::new_v4().to_string(),
            file_path: file_path.clone(),
            file_name: file_name.clone(),
            file_size: metadata.len(),
            s3_key,
            status: UploadStatus::Pending,
            progress: 0.0,
            uploaded_bytes: 0,
            speed_mbps: 0.0,
            eta_seconds: None,
            created_at: chrono::Utc::now().to_rfc3339(),
            started_at: None,
            completed_at: None,
            error_message: None,
            retry_count: 0,
        };
        
        queue.items.push(item);
        added_files.push(file_name);
        
        log::info!("Added file to upload queue: {}", file_path);
    }
    
    if added_files.is_empty() {
        return Err("No valid files were added to the upload queue".to_string());
    }
    
    let message = format!("Added {} files to upload queue: {}", 
                         added_files.len(), 
                         added_files.join(", "));
    
    log::info!("{}", message);
    Ok(message)
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’å‰Šé™¤
#[command]
pub async fn remove_upload_item(
    item_id: String,
    queue_state: State<'_, UploadQueueState>,
) -> Result<String, String> {
    let mut queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å ´åˆã¯ã‚­ãƒ£ãƒ³ã‚»ãƒ«
    if let Some(progress) = queue.active_uploads.get(&item_id) {
        if progress.status == UploadStatus::InProgress {
            // TODO: å®Ÿéš›ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ãƒ³ã‚»ãƒ«å‡¦ç†
            log::info!("Cancelling active upload: {}", item_id);
        }
    }
    
    queue.active_uploads.remove(&item_id);
    
    let initial_len = queue.items.len();
    queue.items.retain(|item| item.id != item_id);
    
    if queue.items.len() == initial_len {
        return Err(format!("Upload item not found: {}", item_id));
    }
    
    log::info!("Removed item from upload queue: {}", item_id);
    Ok("Item removed from upload queue".to_string())
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’é–‹å§‹
#[command]
pub async fn start_upload_processing(
    app_handle: AppHandle,
    queue_state: State<'_, UploadQueueState>,
) -> Result<String, String> {
    let mut queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    if queue.is_processing {
        return Ok("Upload processing is already running".to_string());
    }
    
    let config = queue.config.clone()
        .ok_or("Upload queue not initialized")?;
    
    queue.is_processing = true;
    drop(queue); // ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾
    
    // ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
    log::info!("Sending test event to frontend");
    if let Err(e) = app_handle.emit("test-event", "Upload processing starting") {
        log::error!("Failed to emit test event: {}", e);
    } else {
        log::info!("Test event emitted successfully");
    }
    
    // ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’é–‹å§‹
    let queue_clone = Arc::clone(&queue_state.inner());
    let app_handle_clone = app_handle.clone();
    
    std::thread::spawn(move || {
        let rt = tokio::runtime::Runtime::new().unwrap();
        rt.block_on(async move {
            log::info!("Background upload processing thread started");
            if let Err(e) = process_upload_queue(queue_clone, app_handle_clone, config).await {
                log::error!("Upload processing failed: {}", e);
            }
            log::info!("Background upload processing thread finished");
        });
    });
    
    log::info!("Upload processing started");
    Ok("Upload processing started".to_string())
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’åœæ­¢
#[command]
pub async fn stop_upload_processing(
    queue_state: State<'_, UploadQueueState>,
) -> Result<String, String> {
    let mut queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    queue.is_processing = false;
    
    // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ä¸€æ™‚åœæ­¢
    for (_, progress) in queue.active_uploads.iter_mut() {
        if progress.status == UploadStatus::InProgress {
            progress.status = UploadStatus::Paused;
        }
    }
    
    log::info!("Upload processing stopped");
    Ok("Upload processing stopped".to_string())
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã‚’å–å¾—
#[command]
pub async fn get_upload_queue_status(
    queue_state: State<'_, UploadQueueState>,
) -> Result<UploadStatistics, String> {
    let queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    let total_files = queue.items.len() as u64;
    let completed_files = queue.items.iter()
        .filter(|item| item.status == UploadStatus::Completed)
        .count() as u64;
    let failed_files = queue.items.iter()
        .filter(|item| item.status == UploadStatus::Failed)
        .count() as u64;
    let pending_files = queue.items.iter()
        .filter(|item| item.status == UploadStatus::Pending)
        .count() as u64;
    let in_progress_files = queue.items.iter()
        .filter(|item| item.status == UploadStatus::InProgress)
        .count() as u64;
    
    let total_bytes: u64 = queue.items.iter().map(|item| item.file_size).sum();
    let uploaded_bytes: u64 = queue.items.iter().map(|item| item.uploaded_bytes).sum();
    
    let average_speed_mbps = if !queue.active_uploads.is_empty() {
        queue.active_uploads.values()
            .map(|p| p.speed_mbps)
            .sum::<f64>() / queue.active_uploads.len() as f64
    } else {
        0.0
    };
    
    let estimated_time_remaining = if average_speed_mbps > 0.0 {
        let remaining_bytes = total_bytes - uploaded_bytes;
        let remaining_mb = remaining_bytes as f64 / (1024.0 * 1024.0);
        Some((remaining_mb / average_speed_mbps) as u64)
    } else {
        None
    };
    
    Ok(UploadStatistics {
        total_files,
        completed_files,
        failed_files,
        pending_files,
        in_progress_files,
        total_bytes,
        uploaded_bytes,
        average_speed_mbps,
        estimated_time_remaining,
    })
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã®å…¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
#[command]
pub async fn get_upload_queue_items(
    queue_state: State<'_, UploadQueueState>,
) -> Result<Vec<UploadItem>, String> {
    let queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    Ok(queue.items.clone())
}

/// ç‰¹å®šã®ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œ
#[command]
pub async fn retry_upload_item(
    item_id: String,
    queue_state: State<'_, UploadQueueState>,
) -> Result<String, String> {
    let mut queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    if let Some(item) = queue.items.iter_mut().find(|i| i.id == item_id) {
        if item.status == UploadStatus::Failed {
            item.status = UploadStatus::Pending;
            item.progress = 0.0;
            item.uploaded_bytes = 0;
            item.error_message = None;
            item.retry_count += 1;
            
            log::info!("Retry scheduled for upload item: {}", item_id);
            Ok("Upload retry scheduled".to_string())
        } else {
            Err(format!("Item {} is not in failed state", item_id))
        }
    } else {
        Err(format!("Upload item not found: {}", item_id))
    }
}

/// S3ã‚­ãƒ¼ã‚’ç”Ÿæˆã™ã‚‹
fn generate_s3_key(file_path: &str, config: &S3KeyConfig) -> Result<String, String> {
    let path = Path::new(file_path);
    let file_name = path.file_name()
        .and_then(|s| s.to_str())
        .ok_or("Invalid file name")?;
    
    let mut key_parts = Vec::new();
    
    // ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ ï¼ˆæœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’é™¤å»ï¼‰
    if let Some(prefix) = &config.prefix {
        let clean_prefix = prefix.trim_end_matches('/');
        if !clean_prefix.is_empty() {
            key_parts.push(clean_prefix.to_string());
        }
    }
    
    // æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’è¿½åŠ 
    if config.use_date_folder {
        let now = chrono::Utc::now();
        key_parts.push(now.format("%Y/%m/%d").to_string());
    }
    
    // ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä¿æŒ
            if config.preserve_directory_structure {
            if let Some(parent) = path.parent() {
                if let Some(_parent_str) = parent.to_str() {
                // ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥ä¸‹ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                if let Some(home_dir) = dirs::home_dir() {
                    if let Ok(relative) = path.strip_prefix(&home_dir) {
                        if let Some(parent) = relative.parent() {
                                                         if let Some(parent_str) = parent.to_str() {
                                 key_parts.push(parent_str.to_string());
                            }
                        }
                    }
                }
            }
        }
    }
    
    // ã‚«ã‚¹ã‚¿ãƒ å‘½åãƒ‘ã‚¿ãƒ¼ãƒ³
    let final_name = if let Some(pattern) = &config.custom_naming_pattern {
        // ç°¡å˜ãªãƒ‘ã‚¿ãƒ¼ãƒ³ç½®æ›ï¼ˆæ‹¡å¼µå¯èƒ½ï¼‰
        pattern
            .replace("{filename}", file_name)
            .replace("{timestamp}", &chrono::Utc::now().timestamp().to_string())
            .replace("{uuid}", &Uuid::new_v4().to_string())
    } else {
        file_name.to_string()
    };
    
    key_parts.push(final_name);
    
    Ok(key_parts.join("/"))
}

/// ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã‚’å‡¦ç†
async fn process_upload_queue(
    queue_state: UploadQueueState,
    app_handle: AppHandle,
    config: UploadConfig,
) -> Result<(), String> {
    log::info!("ğŸš€ process_upload_queue started with max_concurrent: {}", config.max_concurrent_uploads);
    
    // é–‹å§‹æ™‚ã®ãƒ†ã‚¹ãƒˆã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
    if let Err(e) = app_handle.emit("test-event", "process_upload_queue started") {
        log::error!("Failed to emit process start test event: {}", e);
    } else {
        log::info!("Process start test event emitted successfully");
    }
    
    let max_concurrent = config.max_concurrent_uploads;
    let (tx, mut rx) = mpsc::channel::<UploadProgress>(100);
    
    loop {
        // å‡¦ç†åœæ­¢ãƒã‚§ãƒƒã‚¯
        {
            let queue = queue_state.lock()
                .map_err(|e| format!("Failed to lock queue: {}", e))?;
            if !queue.is_processing {
                break;
            }
        }
        
        // æ–°ã—ã„ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        let pending_items = {
            let mut queue = queue_state.lock()
                .map_err(|e| format!("Failed to lock queue: {}", e))?;
            
            let current_active = queue.get_active_upload_count();
            
            log::info!("Concurrent upload check: {} active, max: {}", current_active, max_concurrent);
            
            if current_active >= max_concurrent {
                log::info!("Max concurrent uploads reached ({}), waiting...", current_active);
                drop(queue);
                sleep(Duration::from_millis(1000)).await;
                continue;
            }
            
            let available_slots = max_concurrent.saturating_sub(current_active);
            let mut pending = Vec::new();
            
            // ç„¡æ–™ç‰ˆã®å ´åˆã¯ç‰¹ã«å³æ ¼ã«1ã¤ãšã¤å‡¦ç†
            let max_new_uploads = if max_concurrent == 1 {
                if current_active > 0 { 0 } else { 1 }
            } else {
                available_slots
            };
            
            // Pendingã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ï¼ˆå€Ÿç”¨ã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã€IDã®ã¿åé›†ï¼‰
            let pending_item_ids: Vec<String> = queue.items.iter()
                .filter(|item| item.status == UploadStatus::Pending)
                .take(max_new_uploads)
                .map(|item| item.id.clone())
                .collect();
            
            // çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¦ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
            for item_id in pending_item_ids {
                match queue.start_upload(&item_id) {
                    Ok(()) => {
                        if let Some(item) = queue.items.iter().find(|i| i.id == item_id) {
                            log::info!("Starting upload for: {} (slot {}/{})", 
                                       item.file_name, pending.len() + 1, max_new_uploads);
                            pending.push(item.clone());
                            
                            // ç„¡æ–™ç‰ˆã§ã¯ç¢ºå®Ÿã«1ã¤ãšã¤
                            if max_concurrent == 1 {
                                break;
                            }
                        }
                    }
                    Err(e) => {
                        log::error!("Failed to start upload for {}: {}", item_id, e);
                        break;
                    }
                }
            }
            
            if pending.is_empty() {
                log::debug!("No pending items to process (active: {}, max: {})", current_active, max_concurrent);
            }
            
            pending
        };
        
        // æ–°ã—ã„ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        for item in pending_items {
            let queue_state_clone = queue_state.clone();
            let config_clone = config.clone();
            let tx_clone = tx.clone();
            let item_id = item.id.clone();
            let file_name = item.file_name.clone();
            
            tokio::spawn(async move {
                log::info!("ğŸ”„ Starting upload task for: {} ({})", file_name, item_id);
                
                let result = upload_file_to_s3(
                    item.file_path,
                    item.s3_key,
                    config_clone,
                    tx_clone,
                    item_id.clone(),
                ).await;
                
                let (success, error_msg) = match result {
                    Ok(_) => (true, None),
                    Err(e) => (false, Some(e)),
                };
                
                // æ–°ã—ã„çŠ¶æ…‹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’è¨˜éŒ²
                {
                    let mut queue = queue_state_clone.lock().unwrap();
                    // æ—¢ã«å®Œäº†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆé€²æ—æ›´æ–°ã§å…ˆã«å‡¦ç†ã•ã‚ŒãŸå ´åˆï¼‰
                    if let Some(item) = queue.items.iter().find(|i| i.id == item_id) {
                        if item.status == UploadStatus::Completed {
                            log::info!("âœ… Upload already completed by progress update, skipping task cleanup: {}", item_id);
                        } else {
                            log::info!("ğŸ”„ Task completion: calling complete_upload for {}", item_id);
                            queue.complete_upload(&item_id, success, error_msg.clone());
                        }
                    } else {
                        log::warn!("âš ï¸  Upload item not found during task completion: {}", item_id);
                    }
                }
                
                if success {
                    log::info!("Upload task completed successfully: {} ({})", file_name, item_id);
                } else {
                    log::error!("Upload task failed: {} ({}), error: {}", file_name, item_id, error_msg.unwrap_or_default());
                }
            });
        }
        
        // é€²æ—æ›´æ–°ã‚’å‡¦ç†
        let mut progress_received = 0;
        while let Ok(progress) = rx.try_recv() {
            progress_received += 1;
            {
                let mut queue = queue_state.lock()
                    .map_err(|e| format!("Failed to lock queue: {}", e))?;
                queue.active_uploads.insert(progress.item_id.clone(), progress.clone());
                
                // ã‚­ãƒ¥ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ ã®é€²æ—ã‚‚æ›´æ–°
                let (should_cleanup, file_name, file_size, is_success) = {
                    if let Some(item) = queue.items.iter_mut().find(|i| i.id == progress.item_id) {
                        let was_in_progress = item.status == UploadStatus::InProgress;
                        
                        item.progress = progress.percentage;
                        item.uploaded_bytes = progress.uploaded_bytes;
                        item.speed_mbps = progress.speed_mbps;
                        item.eta_seconds = progress.eta_seconds;
                        item.status = progress.status.clone();
                        
                        // å®Œäº†æ™‚ï¼ˆæˆåŠŸãƒ»å¤±æ•—å•ã‚ãšï¼‰ã®åˆ¤å®šã¨å¿…è¦ãªå€¤ã®å–å¾—
                        let is_completed = matches!(progress.status, UploadStatus::Completed | UploadStatus::Failed);
                        if is_completed && was_in_progress {
                            if progress.status == UploadStatus::Completed {
                                item.completed_at = Some(chrono::Utc::now().to_rfc3339());
                            }
                            (true, item.file_name.clone(), item.file_size, progress.status == UploadStatus::Completed)
                        } else {
                            (false, String::new(), 0, false)
                        }
                    } else {
                        (false, String::new(), 0, false)
                    }
                };
                
                // å€Ÿç”¨ãŒçµ‚äº†ã—ãŸå¾Œã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
                if should_cleanup {
                    if is_success {
                        log::info!("ğŸ‰ Upload 100% completed, performing immediate cleanup: {}", progress.item_id);
                        queue.total_files_uploaded += 1;
                        queue.total_uploaded_bytes += file_size;
                    } else {
                        log::info!("ğŸ’¥ Upload failed, performing immediate cleanup: {}", progress.item_id);
                    }
                    
                    // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚«ã‚¦ãƒ³ãƒˆã‚’æ¸›ã‚‰ã™
                    let old_count = queue.active_upload_count;
                    if queue.active_upload_count > 0 {
                        queue.active_upload_count -= 1;
                        log::info!("ğŸ”½ Active upload count decreased: {} -> {}", 
                                   old_count, queue.active_upload_count);
                    }
                    
                    // active_uploadsã‹ã‚‰å‰Šé™¤
                    let removed = queue.active_uploads.remove(&progress.item_id);
                    log::info!("ğŸ—‘ï¸  Removed from active_uploads: {} (was present: {})", progress.item_id, removed.is_some());
                    
                    if is_success {
                        log::info!("âœ… Upload completed and cleaned up: {} ({})", file_name, progress.item_id);
                    } else {
                        log::info!("âŒ Upload failed and cleaned up: {} ({})", file_name, progress.item_id);
                    }
                    log::info!("ğŸ“Š Cleanup summary - Active count: {}, Active uploads: {}", 
                               queue.active_upload_count, queue.active_uploads.len());
                }
            }
            
            // ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«é€²æ—ã‚’é€šçŸ¥
            log::info!("Emitting progress event to frontend: {:.1}% for {}", 
                       progress.percentage, progress.item_id);
            if let Err(e) = app_handle.emit("upload-progress", &progress) {
                log::error!("Failed to emit upload progress: {}", e);
            } else {
                log::info!("Progress event emitted successfully: {:.1}%", progress.percentage);
            }
        }
        
        if progress_received > 0 {
            log::info!("Processed {} progress updates in this cycle", progress_received);
        }
        
        // ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åœæ­¢æ¤œå‡ºã¨ãƒªã‚«ãƒãƒª
        let (has_pending, has_active, all_completed) = {
            let queue = queue_state.lock()
                .map_err(|e| format!("Failed to lock queue: {}", e))?;
            let pending = queue.items.iter().any(|item| item.status == UploadStatus::Pending);
            let active = queue.get_active_upload_count() > 0;
            let completed = queue.items.iter().all(|item| 
                matches!(item.status, UploadStatus::Completed | UploadStatus::Failed | UploadStatus::Cancelled)
            );
            (pending, active, completed)
        };
        
        // å…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå®Œäº†ã—ãŸå ´åˆã¯å‡¦ç†ã‚’åœæ­¢
        if all_completed && !has_active {
            log::info!("All uploads completed, stopping upload processing");
            let mut queue = queue_state.lock()
                .map_err(|e| format!("Failed to lock queue: {}", e))?;
            queue.is_processing = false;
            break;
        }
        
        if has_pending && !has_active {
            log::warn!("Upload seems stalled: pending items found but no active uploads");
        }
        
        sleep(Duration::from_millis(500)).await;
    }
    
    Ok(())
}

/// å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†
async fn upload_file_to_s3(
    file_path: String,
    s3_key: String,
    config: UploadConfig,
    progress_tx: mpsc::Sender<UploadProgress>,
    item_id: String,
) -> Result<String, String> {
    use aws_config::{BehaviorVersion, Region};
    use aws_sdk_s3::Client as S3Client;
    use aws_credential_types::Credentials;
    use std::path::Path;
    use tokio::fs::File;
    use tokio::io::AsyncReadExt;
    
    log::info!("Starting upload for file: {} -> s3://{}/{}", file_path, config.bucket_name, s3_key);
    
    // ğŸ” å—ä¿¡ã—ãŸè¨­å®šã®å…¨è©³ç´°ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    log::info!("ğŸ”§ === ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨­å®šè©³ç´° ===");
    log::info!("ğŸ”§ tier: {:?}", config.tier);
    log::info!("ğŸ”§ chunk_size_mb: {}", config.chunk_size_mb);
    log::info!("ğŸ”§ max_concurrent_uploads: {}", config.max_concurrent_uploads);
    log::info!("ğŸ”§ max_concurrent_parts: {}", config.max_concurrent_parts);
    log::info!("ğŸ”§ adaptive_chunk_size: {}", config.adaptive_chunk_size);
    log::info!("ğŸ”§ min_chunk_size_mb: {}", config.min_chunk_size_mb);
    log::info!("ğŸ”§ max_chunk_size_mb: {}", config.max_chunk_size_mb);
    log::info!("ğŸ”§ retry_attempts: {}", config.retry_attempts);
    log::info!("ğŸ”§ timeout_seconds: {}", config.timeout_seconds);
    log::info!("ğŸ”§ enable_resume: {}", config.enable_resume);
    log::info!("ğŸ”§ ========================");
    
    // ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    let path = Path::new(&file_path);
    if !path.exists() {
        return Err(format!("File does not exist: {}", file_path));
    }
    
    let file_size = std::fs::metadata(&path)
        .map_err(|e| format!("Failed to get file metadata: {}", e))?
        .len();
    
    // AWSè¨­å®š
    let region = Region::new(config.aws_credentials.region.clone());
    let mut config_builder = aws_config::defaults(BehaviorVersion::latest())
        .region(region);
    
    let creds = Credentials::new(
        &config.aws_credentials.access_key_id,
        &config.aws_credentials.secret_access_key,
        config.aws_credentials.session_token.clone(),
        None,
        "upload",
    );
    
    config_builder = config_builder.credentials_provider(creds);
    let aws_config = config_builder.load().await;
    let s3_client = S3Client::new(&aws_config);
    
    let start_time = Instant::now();
    let mut uploaded_bytes = 0u64;
    
    // é€²æ—ãƒ¬ãƒãƒ¼ãƒˆç”¨ã®ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£
    let report_progress = |uploaded: u64, total: u64, speed_mbps: f64| {
        let percentage = if total > 0 {
            (uploaded as f64 / total as f64) * 100.0
        } else {
            0.0
        };
        
        let eta_seconds = if speed_mbps > 0.0 {
            let remaining_mb = (total - uploaded) as f64 / (1024.0 * 1024.0);
            Some((remaining_mb / speed_mbps) as u64)
        } else {
            None
        };
        
        let progress = UploadProgress {
            item_id: item_id.clone(),
            uploaded_bytes: uploaded,
            total_bytes: total,
            percentage,
            speed_mbps,
            eta_seconds,
            status: if uploaded >= total {
                UploadStatus::Completed
            } else {
                UploadStatus::InProgress
            },
        };
        
        if let Err(e) = progress_tx.try_send(progress) {
            log::warn!("Failed to send progress update: {}", e);
        }
    };
    
    // S3åˆ¶é™æº–æ‹ ã®ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨­å®šï¼ˆäº‹å‰è¨ˆç®—ï¼‰
    let configured_size = config.chunk_size_mb * 1024 * 1024;
    let s3_min_size = 5 * 1024 * 1024; // 5MB
    let effective_chunk_size = std::cmp::max(configured_size, s3_min_size);
    
    // ğŸ” ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—ã®è©³ç´°ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    log::info!("ğŸ”§ === ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨ˆç®— ===");
    log::info!("ğŸ”§ config.chunk_size_mb: {}", config.chunk_size_mb);
    log::info!("ğŸ”§ configured_size (bytes): {}", configured_size);
    log::info!("ğŸ”§ s3_min_size (bytes): {}", s3_min_size);
    log::info!("ğŸ”§ effective_chunk_size (bytes): {}", effective_chunk_size);
    log::info!("ğŸ”§ effective_chunk_size (MB): {}", effective_chunk_size / (1024 * 1024));
    log::info!("ğŸ”§ ========================");
    
    if effective_chunk_size != configured_size {
        log::warn!("âš ï¸ Chunk size adjusted for S3 compliance: {} MB -> {} MB", 
                  configured_size / (1024 * 1024), effective_chunk_size / (1024 * 1024));
    }
    
    // å°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å˜ç´”ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆèª¿æ•´å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã§åˆ¤å®šï¼‰
    if file_size <= effective_chunk_size {
        log::info!("Using simple upload for small file: {} bytes", file_size);
        
        let mut file = File::open(&path).await
            .map_err(|e| format!("Failed to open file: {}", e))?;
        
        let mut buffer = Vec::new();
        file.read_to_end(&mut buffer).await
            .map_err(|e| format!("Failed to read file: {}", e))?;
        
        uploaded_bytes = buffer.len() as u64;
        
        // é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ
        let elapsed = start_time.elapsed().as_secs_f64();
        let speed_mbps = if elapsed > 0.0 {
            (uploaded_bytes as f64 / (1024.0 * 1024.0)) / elapsed
        } else {
            0.0
        };
        
        report_progress(uploaded_bytes, file_size, speed_mbps);
        
        s3_client
            .put_object()
            .bucket(&config.bucket_name)
            .key(&s3_key)
            .body(buffer.into())
            .send()
            .await
            .map_err(|e| format!("S3 upload failed: {}", e))?;
        
        log::info!("Simple upload completed: {} bytes", uploaded_bytes);
        
    } else {
        // ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        log::info!("Using multipart upload for large file: {} bytes", file_size);
        
        let create_response = s3_client
            .create_multipart_upload()
            .bucket(&config.bucket_name)
            .key(&s3_key)
            .send()
            .await
            .map_err(|e| format!("Failed to create multipart upload: {}", e))?;
        
        let upload_id = create_response.upload_id()
            .ok_or("No upload ID returned")?;
        
        // äº‹å‰è¨ˆç®—ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
        let chunk_size = effective_chunk_size;
        let mut part_number = 1;
        let mut completed_parts = Vec::new();
        
        let mut file = File::open(&path).await
            .map_err(|e| format!("Failed to open file: {}", e))?;
        
        loop {
            let mut buffer = vec![0u8; chunk_size as usize];
            
            // ğŸ” ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            log::info!("ğŸ”§ Reading chunk: buffer_size={} bytes ({} MB)", 
                       buffer.len(), buffer.len() / (1024 * 1024));
            
            // å®Œå…¨ã«ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’èª­ã¿è¾¼ã‚€ã¾ã§ãƒ«ãƒ¼ãƒ—
            let mut total_bytes_read = 0;
            let mut temp_buffer = vec![0u8; chunk_size as usize];
            
            while total_bytes_read < chunk_size as usize {
                let bytes_read = file.read(&mut temp_buffer[total_bytes_read..]).await
                    .map_err(|e| format!("Failed to read file chunk: {}", e))?;
                
                if bytes_read == 0 {
                    // ãƒ•ã‚¡ã‚¤ãƒ«çµ‚ç«¯ã«é”ã—ãŸ
                    break;
                }
                
                total_bytes_read += bytes_read;
            }
            
            // ğŸ” å®Ÿéš›ã®èª­ã¿è¾¼ã¿ã‚µã‚¤ã‚ºã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            log::info!("ğŸ”§ Read result: total_bytes_read={} bytes ({} MB)", 
                       total_bytes_read, total_bytes_read / (1024 * 1024));
            
            if total_bytes_read == 0 {
                break;
            }
            
            // å®Ÿéš›ã«èª­ã¿è¾¼ã‚“ã ã‚µã‚¤ã‚ºã§ãƒãƒƒãƒ•ã‚¡ã‚’èª¿æ•´
            temp_buffer.truncate(total_bytes_read);
            buffer = temp_buffer;
            
            let upload_part_response = s3_client
                .upload_part()
                .bucket(&config.bucket_name)
                .key(&s3_key)
                .upload_id(upload_id)
                .part_number(part_number)
                .body(buffer.into())
                .send()
                .await
                .map_err(|e| format!("Failed to upload part {}: {}", part_number, e))?;
            
            let etag = upload_part_response.e_tag()
                .ok_or(format!("No ETag for part {}", part_number))?;
            
            completed_parts.push(
                aws_sdk_s3::types::CompletedPart::builder()
                    .part_number(part_number)
                    .e_tag(etag)
                    .build()
            );
            
            uploaded_bytes += total_bytes_read as u64;
            part_number += 1;
            
            // é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ
            let elapsed = start_time.elapsed().as_secs_f64();
            let speed_mbps = if elapsed > 0.0 {
                (uploaded_bytes as f64 / (1024.0 * 1024.0)) / elapsed
            } else {
                0.0
            };
            
            report_progress(uploaded_bytes, file_size, speed_mbps);
            
            log::info!("Uploaded part {}: {} bytes (total: {}/{})", 
                       part_number - 1, total_bytes_read, uploaded_bytes, file_size);
        }
        
        // ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
        log::info!("ğŸ”§ Completing multipart upload with {} parts", completed_parts.len());
        
        // ãƒ‘ãƒ¼ãƒ„ã‚’éƒ¨å“ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé‡è¦ï¼‰
        let mut sorted_parts = completed_parts;
        sorted_parts.sort_by_key(|part| part.part_number());
        
        // ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆè©³ç´°ï¼‰
        log::info!("ğŸ”§ Preparing to complete multipart upload with {} parts:", sorted_parts.len());
        for (i, part) in sorted_parts.iter().enumerate() {
            log::info!("  Part {}: number={:?}, etag={:?}", 
                       i + 1, part.part_number(), part.e_tag());
            
            // ãƒ‘ãƒ¼ãƒ„ç•ªå·ã®æ¤œè¨¼
            if part.part_number().is_none() {
                log::error!("âŒ Part {} has None part_number!", i + 1);
            }
            if part.e_tag().is_none() {
                log::error!("âŒ Part {} has None etag!", i + 1);
            }
        }
        
        let parts_count = sorted_parts.len();
        let completed_upload = aws_sdk_s3::types::CompletedMultipartUpload::builder()
            .set_parts(Some(sorted_parts))
            .build();
        
        // ãƒªãƒˆãƒ©ã‚¤ä»˜ããƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆå®Œäº†
        let mut retry_count = 0;
        let max_retries = 3;
        
        loop {
            match s3_client
                .complete_multipart_upload()
                .bucket(&config.bucket_name)
                .key(&s3_key)
                .upload_id(upload_id)
                .multipart_upload(completed_upload.clone())
                .send()
                .await
            {
                Ok(response) => {
                    log::info!("âœ… Multipart upload completed successfully: {:?}", response.location());
                    break;
                }
                Err(e) => {
                    retry_count += 1;
                    
                    // è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
                    log::error!("ğŸ” Multipart upload completion error details:");
                    log::error!("  â”œâ”€ Error: {:?}", e);
                    log::error!("  â”œâ”€ Bucket: {}", config.bucket_name);
                    log::error!("  â”œâ”€ Key: {}", s3_key);
                    log::error!("  â”œâ”€ Upload ID: {}", upload_id);
                    log::error!("  â”œâ”€ Parts count: {}", parts_count);
                    log::error!("  â””â”€ Attempt: {}/{}", retry_count, max_retries);
                    
                    if retry_count > max_retries {
                        log::error!("âŒ Multipart upload completion failed after {} retries: {}", max_retries, e);
                        return Err(format!("Failed to complete multipart upload after {} retries: {}", max_retries, e));
                    }
                    
                    let delay = Duration::from_millis(1000 * retry_count as u64);
                    log::warn!("âš ï¸ Multipart upload completion failed (attempt {}), retrying in {:?}: {}", 
                              retry_count, delay, e);
                    tokio::time::sleep(delay).await;
                }
            }
        }
        
        log::info!("Multipart upload completed: {} bytes in {} parts", uploaded_bytes, part_number - 1);
    }
    
    // æœ€çµ‚é€²æ—ãƒ¬ãƒãƒ¼ãƒˆ
    let elapsed = start_time.elapsed().as_secs_f64();
    let speed_mbps = if elapsed > 0.0 {
        (uploaded_bytes as f64 / (1024.0 * 1024.0)) / elapsed
    } else {
        0.0
    };
    
    report_progress(uploaded_bytes, file_size, speed_mbps);
    
    // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if config.auto_create_metadata {
        use std::collections::HashMap;
        let tags = vec!["upload".to_string()];
        let custom_fields = HashMap::new();
        if let Err(e) = create_file_metadata(file_path.clone(), tags, custom_fields).await {
            log::warn!("Failed to create metadata for {}: {}", s3_key, e);
        }
    }
    
    Ok(format!("Upload completed: {} bytes", uploaded_bytes))
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢
#[command]
pub async fn clear_upload_queue(
    queue_state: State<'_, UploadQueueState>,
) -> Result<String, String> {
    let mut queue = queue_state.lock()
        .map_err(|e| format!("Failed to lock upload queue: {}", e))?;
    
    // ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯åœæ­¢
    if !queue.active_uploads.is_empty() {
        queue.is_processing = false;
    }
    
    queue.items.clear();
    queue.active_uploads.clear();
    
    log::info!("Upload queue cleared");
    Ok("Upload queue cleared".to_string())
}

/// ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨­å®šã‚’ãƒ†ã‚¹ãƒˆ
#[command]
pub async fn test_upload_config(config: UploadConfig) -> Result<String, String> {
    use aws_config::{BehaviorVersion, Region};
    use aws_sdk_s3::Client as S3Client;
    use aws_credential_types::Credentials;
    
    // AWSèªè¨¼ãƒ†ã‚¹ãƒˆ
    let region = Region::new(config.aws_credentials.region.clone());
    let mut config_builder = aws_config::defaults(BehaviorVersion::latest())
        .region(region);
    
    let creds = Credentials::new(
        &config.aws_credentials.access_key_id,
        &config.aws_credentials.secret_access_key,
        config.aws_credentials.session_token.clone(),
        None,
        "test",
    );
    
    config_builder = config_builder.credentials_provider(creds);
    let aws_config = config_builder.load().await;
    let s3_client = S3Client::new(&aws_config);
    
    // ãƒã‚±ãƒƒãƒˆã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
    s3_client
        .head_bucket()
        .bucket(&config.bucket_name)
        .send()
        .await
        .map_err(|e| format!("Bucket access test failed: {}", e))?;
    
    Ok(format!("Upload configuration test successful for bucket: {}", config.bucket_name))
}




#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashMap;
    use tempfile::tempdir;
    use std::fs::File;
    use std::io::Write;

    fn create_test_credentials() -> AwsCredentials {
        AwsCredentials {
            access_key_id: "test_access_key".to_string(),
            secret_access_key: "test_secret_key".to_string(),
            region: "ap-northeast-1".to_string(),
            session_token: None,
        }
    }

    fn create_test_upload_config() -> UploadConfig {
        UploadConfig::new(
            create_test_credentials(),
            "test-bucket".to_string(),
            UploadTier::Premium
        )
    }

    fn create_test_s3_key_config() -> S3KeyConfig {
        S3KeyConfig {
            prefix: Some("test".to_string()),
            use_date_folder: true,
            preserve_directory_structure: false,
            custom_naming_pattern: None,
        }
    }

    #[test]
    fn test_upload_queue_creation() {
        let queue = UploadQueue::new();
        assert_eq!(queue.items.len(), 0);
        assert_eq!(queue.active_uploads.len(), 0);
        assert!(!queue.is_processing);
        assert_eq!(queue.total_uploaded_bytes, 0);
        assert_eq!(queue.total_files_uploaded, 0);
    }

    #[test]
    fn test_s3_key_generation_simple() {
        let config = S3KeyConfig {
            prefix: Some("uploads".to_string()),
            use_date_folder: false,
            preserve_directory_structure: false,
            custom_naming_pattern: None,
        };

        let result = generate_s3_key("/path/to/test.mp4", &config).unwrap();
        assert_eq!(result, "uploads/test.mp4");
    }

    #[test]
    fn test_s3_key_generation_with_date() {
        let config = S3KeyConfig {
            prefix: Some("media".to_string()),
            use_date_folder: true,
            preserve_directory_structure: false,
            custom_naming_pattern: None,
        };

        let result = generate_s3_key("/path/to/video.mov", &config).unwrap();
        assert!(result.starts_with("media/"));
        assert!(result.contains("/video.mov"));
        // æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆYYYY/MM/DDå½¢å¼ï¼‰
        let parts: Vec<&str> = result.split('/').collect();
        assert!(parts.len() >= 4); // media/YYYY/MM/DD/video.mov
    }

    #[test]
    fn test_s3_key_generation_custom_pattern() {
        let config = S3KeyConfig {
            prefix: None,
            use_date_folder: false,
            preserve_directory_structure: false,
            custom_naming_pattern: Some("{timestamp}_{filename}".to_string()),
        };

        let result = generate_s3_key("/path/to/test.mp4", &config).unwrap();
        assert!(result.contains("_test.mp4"));
        assert!(result.len() > "test.mp4".len()); // ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹
    }

    #[tokio::test]
    async fn test_upload_queue_initialization() {
        let queue = Arc::new(Mutex::new(UploadQueue::new()));
        let config = create_test_upload_config();

        // åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã®AWSæ¥ç¶šã¯è¡Œã‚ãªã„ï¼‰
        {
            let mut q = queue.lock().unwrap();
            q.config = Some(config.clone());
        }

        let q = queue.lock().unwrap();
        assert!(q.config.is_some());
        assert_eq!(q.config.as_ref().unwrap().bucket_name, "test-bucket");
    }

    #[tokio::test]
    async fn test_file_addition_to_queue() {
        let temp_dir = tempdir().unwrap();
        let file_path = temp_dir.path().join("test_video.mp4");
        
        // ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        {
            let mut file = File::create(&file_path).unwrap();
            file.write_all(b"test video content").unwrap();
        }

        let queue = Arc::new(Mutex::new(UploadQueue::new()));
        let s3_key_config = create_test_s3_key_config();

        // ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        let file_path_str = file_path.to_string_lossy().to_string();
        let s3_key = generate_s3_key(&file_path_str, &s3_key_config).unwrap();

        let item = UploadItem {
            id: Uuid::new_v4().to_string(),
            file_path: file_path_str.clone(),
            file_name: "test_video.mp4".to_string(),
            file_size: 18, // "test video content".len()
            s3_key: s3_key.clone(),
            status: UploadStatus::Pending,
            progress: 0.0,
            uploaded_bytes: 0,
            speed_mbps: 0.0,
            eta_seconds: None,
            created_at: chrono::Utc::now().to_rfc3339(),
            started_at: None,
            completed_at: None,
            error_message: None,
            retry_count: 0,
        };

        {
            let mut q = queue.lock().unwrap();
            q.items.push(item);
        }

        let q = queue.lock().unwrap();
        assert_eq!(q.items.len(), 1);
        assert_eq!(q.items[0].file_name, "test_video.mp4");
        assert_eq!(q.items[0].status, UploadStatus::Pending);
        assert!(q.items[0].s3_key.contains("test_video.mp4"));
    }

    #[test]
    fn test_upload_statistics_calculation() {
        let mut queue = UploadQueue::new();

        // ãƒ†ã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‚’è¿½åŠ 
        for i in 0..5 {
            let item = UploadItem {
                id: format!("item_{}", i),
                file_path: format!("/test/file_{}.mp4", i),
                file_name: format!("file_{}.mp4", i),
                file_size: 1000,
                s3_key: format!("uploads/file_{}.mp4", i),
                status: if i < 2 { UploadStatus::Completed } else if i < 4 { UploadStatus::Pending } else { UploadStatus::Failed },
                progress: if i < 2 { 100.0 } else { 0.0 },
                uploaded_bytes: if i < 2 { 1000 } else { 0 },
                speed_mbps: 0.0,
                eta_seconds: None,
                created_at: chrono::Utc::now().to_rfc3339(),
                started_at: None,
                completed_at: None,
                error_message: None,
                retry_count: 0,
            };
            queue.items.push(item);
        }

        let total_files = queue.items.len() as u64;
        let completed_files = queue.items.iter()
            .filter(|item| item.status == UploadStatus::Completed)
            .count() as u64;
        let failed_files = queue.items.iter()
            .filter(|item| item.status == UploadStatus::Failed)
            .count() as u64;
        let pending_files = queue.items.iter()
            .filter(|item| item.status == UploadStatus::Pending)
            .count() as u64;

        assert_eq!(total_files, 5);
        assert_eq!(completed_files, 2);
        assert_eq!(failed_files, 1);
        assert_eq!(pending_files, 2);
    }

    #[test]
    fn test_upload_item_status_transitions() {
        let mut item = UploadItem {
            id: "test_item".to_string(),
            file_path: "/test/file.mp4".to_string(),
            file_name: "file.mp4".to_string(),
            file_size: 1000,
            s3_key: "uploads/file.mp4".to_string(),
            status: UploadStatus::Pending,
            progress: 0.0,
            uploaded_bytes: 0,
            speed_mbps: 0.0,
            eta_seconds: None,
            created_at: chrono::Utc::now().to_rfc3339(),
            started_at: None,
            completed_at: None,
            error_message: None,
            retry_count: 0,
        };

        // Pending -> InProgress
        item.status = UploadStatus::InProgress;
        item.started_at = Some(chrono::Utc::now().to_rfc3339());
        assert_eq!(item.status, UploadStatus::InProgress);
        assert!(item.started_at.is_some());

        // InProgress -> Completed
        item.status = UploadStatus::Completed;
        item.progress = 100.0;
        item.uploaded_bytes = 1000;
        item.completed_at = Some(chrono::Utc::now().to_rfc3339());
        assert_eq!(item.status, UploadStatus::Completed);
        assert_eq!(item.progress, 100.0);
        assert_eq!(item.uploaded_bytes, 1000);
        assert!(item.completed_at.is_some());
    }

    #[test]
    fn test_upload_progress_calculation() {
        let uploaded_bytes = 500u64;
        let total_bytes = 1000u64;
        let percentage = (uploaded_bytes as f64 / total_bytes as f64) * 100.0;

        assert_eq!(percentage, 50.0);

        // é€Ÿåº¦è¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ
        let elapsed_seconds = 10.0;
        let speed_mbps = (uploaded_bytes as f64 / (1024.0 * 1024.0)) / elapsed_seconds;
        assert!(speed_mbps > 0.0);

        // ETAè¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ
        if speed_mbps > 0.0 {
            let remaining_mb = (total_bytes - uploaded_bytes) as f64 / (1024.0 * 1024.0);
            let eta_seconds = (remaining_mb / speed_mbps) as u64;
            assert!(eta_seconds > 0);
        }
    }

    #[test]
    fn test_upload_config_validation() {
        let config = create_test_upload_config();
        
        assert!(!config.aws_credentials.access_key_id.is_empty());
        assert!(!config.aws_credentials.secret_access_key.is_empty());
        assert!(!config.aws_credentials.region.is_empty());
        assert!(!config.bucket_name.is_empty());
        assert!(config.max_concurrent_uploads > 0);
        assert!(config.chunk_size_mb > 0);
        assert!(config.retry_attempts > 0);
        assert!(config.timeout_seconds > 0);
    }

    #[test]
    fn test_s3_key_config_presets() {
        // Simple preset
        let simple = S3KeyConfig {
            prefix: Some("uploads".to_string()),
            use_date_folder: false,
            preserve_directory_structure: false,
            custom_naming_pattern: None,
        };
        let key = generate_s3_key("/test/file.mp4", &simple).unwrap();
        assert_eq!(key, "uploads/file.mp4");

        // Organized preset
        let organized = S3KeyConfig {
            prefix: Some("media".to_string()),
            use_date_folder: true,
            preserve_directory_structure: false,
            custom_naming_pattern: None,
        };
        let key = generate_s3_key("/test/file.mp4", &organized).unwrap();
        assert!(key.starts_with("media/"));
        assert!(key.ends_with("/file.mp4"));
    }
}